{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import dirname, join as pjoin\n",
    "import scipy.io as sio\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "from func2graph import data, baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "1 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "2 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "3 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "4 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "5 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "6 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "7 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "8 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "9 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "10 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "11 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "tensor([0.4689, 0.9265, 0.4217, 0.1391, 0.4735, 0.7944, 0.0489, 0.8042, 0.6423,\n",
      "        0.4617, 0.3809, 0.0762, 0.6617])\n",
      "tensor([0.7390, 0.3785, 0.2016, 0.1473, 0.6801, 0.2504, 0.8860, 0.9147, 0.8447,\n",
      "        0.7335, 0.8857, 0.9761, 0.1897])\n",
      "tensor([0.4689, 0.9265, 0.4217, 0.1391, 0.4735, 0.7944, 0.0489, 0.8042, 0.6423,\n",
      "        0.4617, 0.3809, 0.0762, 0.6617])\n",
      "tensor([0.7390, 0.3785, 0.2016, 0.1473, 0.6801, 0.2504, 0.8860, 0.9147, 0.8447,\n",
      "        0.7335, 0.8857, 0.9761, 0.1897])\n",
      "12 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "13 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "14 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "15 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "16 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "17 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "18 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "19 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n"
     ]
    }
   ],
   "source": [
    "# sessions are preprocessed by sliding windows, num_unqiue_cell_types and num_unique_neurons are known\n",
    "session1 = torch.rand(32, 10, 100)\n",
    "session2 = torch.rand(30, 13, 100)\n",
    "session1_cell_type_ids = torch.rand(32, 10)\n",
    "session2_cell_type_ids = torch.rand(30, 13)\n",
    "session1_neuron_ids = torch.rand(32, 10)\n",
    "session2_neuron_ids = torch.rand(30, 13)\n",
    "\n",
    "all_sessions = [session1, session2]\n",
    "all_sessions_cell_type_ids = [session1_cell_type_ids, session2_cell_type_ids]\n",
    "all_sessions_neuron_ids = [session1_neuron_ids, session2_neuron_ids]\n",
    "\n",
    "class Mouse_Session_Dataset(TensorDataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        all_sessions, \n",
    "        all_sessions_cell_type_ids, \n",
    "        all_sessions_neuron_ids,\n",
    "        batch_size=3,                      # real batch size\n",
    "    ):\n",
    "        num_batch_per_session = [session.shape[0] // batch_size for session in all_sessions]\n",
    "\n",
    "        self.all_batch = []\n",
    "        self.all_batch_cell_type_ids = []\n",
    "        self.all_batch_neuron_ids = []\n",
    "        for i in range(len(num_batch_per_session)):      # for each session\n",
    "            for j in range(num_batch_per_session[i]):      # for each batch\n",
    "                self.all_batch.append(all_sessions[i][j*batch_size:(j+1)*batch_size])\n",
    "                self.all_batch_cell_type_ids.append(all_sessions_cell_type_ids[i][j*batch_size:(j+1)*batch_size])\n",
    "                self.all_batch_neuron_ids.append(all_sessions_neuron_ids[i][j*batch_size:(j+1)*batch_size])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.all_batch[index], self.all_batch_cell_type_ids[index], self.all_batch_neuron_ids[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_batch)\n",
    "\n",
    "\n",
    "\n",
    "dataset = Session_Dataset(all_sessions, all_sessions_cell_type_ids, all_sessions_neuron_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)    # this is not real batch_size\n",
    "for idx, (batch, batch_cell_type_ids, batch_neuron_ids) in enumerate(dataloader):\n",
    "    batch = batch.squeeze(0)                 # remove the fake batch_size\n",
    "    batch_cell_type_ids = batch_cell_type_ids.squeeze(0)\n",
    "    batch_neuron_ids = batch_neuron_ids.squeeze(0)\n",
    "    print(idx, batch.shape, batch_cell_type_ids.shape, batch_neuron_ids.shape)\n",
    "\n",
    "    if idx == 11:\n",
    "        print(batch_cell_type_ids[1])\n",
    "        print(batch_neuron_ids[1])\n",
    "        print(session2_cell_type_ids[4])\n",
    "        print(session2_neuron_ids[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "  28.  29.  30.  31.  32.  33.  34.  34.  35.  35.  36.  37.  38.  39.\n",
      "  40.  41.  41.  41.  42.  43.  44.  45.  45.  45.  45.  46.  47.  47.\n",
      "  48.  48.  48.  48.  49.  50.  51.  52.  53.  53.  53.  53.  54.  55.\n",
      "  56.  57.  58.  58.  59.  59.  60.  60.  61.  61.  62.  63.  64.  65.\n",
      "  65.  65.  66.  67.  68.  69.  70.  71.  72.  72.  73.  74.  75.  75.\n",
      "  76.  77.  78.  79.  79.  80.  81.  82.  82.  83.  83.  84.  85.  85.\n",
      "  85.  85.  86.  87.  87.  88.  88.  89.  90.  91.  91.  92.  92.  93.\n",
      "  93.  94.  95.  96.  96.  96.  97.  98.  98.  98.  98.  99. 100. 100.\n",
      " 101. 101. 101. 102. 102. 102. 103. 103. 103. 104. 105. 106. 107. 108.\n",
      " 108. 108. 109. 110. 110. 111. 111. 112. 113. 114. 115. 116. 117. 117.\n",
      " 117. 117. 118. 118. 118. 119. 120. 120. 121. 122. 123. 124. 125. 126.\n",
      " 127. 127. 128. 129. 130. 131. 131. 132. 132. 133. 133. 133. 134. 134.\n",
      " 135. 136. 137. 138. 139. 139. 139. 140. 140. 141. 142. 142. 143. 144.\n",
      " 145. 146. 146. 147. 147. 148. 149. 149. 149. 150. 151. 152. 152. 153.\n",
      " 153. 154. 154. 154. 154. 155. 156. 157. 157. 158. 158. 158. 159. 160.\n",
      " 161. 162. 163. 163. 164. 165. 166. 166. 166. 167. 168. 169. 169. 170.\n",
      " 170. 171. 171. 172. 172. 172. 173. 173. 174. 175. 176. 177. 177. 177.\n",
      " 177. 178. 179. 180. 181. 181. 182. 183. 184. 184. 184. 184. 185. 186.\n",
      " 187. 187. 187. 188. 189. 190. 190. 191. 192. 192. 193. 194. 194. 195.\n",
      " 196. 197. 197. 198. 199. 200. 201. 201. 202. 203. 204. 204. 205. 205.\n",
      " 205. 206. 207. 208. 208. 209. 210. 210. 211. 212. 212. 212. 213. 213.\n",
      " 213. 214. 215. 215. 215. 216. 216. 217. 217. 217. 218. 219. 220. 220.\n",
      " 221. 222. 223. 223. 224. 225. 225. 226. 226. 227. 227. 228. 228. 229.\n",
      " 230. 231. 232. 233. 234. 235. 236. 237. 237. 238. 239. 240. 240. 240.\n",
      " 241. 242. 242. 242. 243. 244. 244. 245. 246. 246. 247. 248. 248. 249.\n",
      " 250. 251. 252. 252. 252. 253. 254. 255. 256. 257. 258. 259. 260. 260.\n",
      " 261. 261. 262. 263. 264. 265. 266. 267. 268. 269. 269. 270. 270. 271.\n",
      " 272. 273. 273. 274. 275. 275. 275. 276. 277. 278. 279. 280. 281. 282.\n",
      " 283. 284. 285. 285. 286. 287. 287. 288. 288. 288. 289. 289. 289. 290.\n",
      " 291. 292. 293. 294. 295. 296. 296. 297. 298. 299. 299. 299. 300. 300.\n",
      " 301. 301. 302. 303. 304. 304. 304. 304. 305. 305. 305. 306. 306. 306.\n",
      " 306. 307. 308. 308. 309. 310. 311. 312. 312. 312. 313. 314. 314. 314.\n",
      " 314. 315. 316. 317. 318. 318. 318. 318. 319. 320. 321. 321. 322. 322.\n",
      " 323. 324. 325. 326. 326. 326. 327. 327. 327. 328. 329. 329. 330. 330.\n",
      " 331. 332. 333. 334. 335. 336. 337. 337. 337. 338. 338. 339. 340. 341.\n",
      " 342. 343. 344. 345. 346. 347. 348. 349. 350. 350. 351. 351. 352. 352.\n",
      " 352. 353. 354. 355. 356. 356. 357. 357. 358. 358. 359. 360. 361. 362.\n",
      " 363. 363. 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375.\n",
      " 376. 377. 378. 379. 379. 380. 380. 381. 382. 383. 383. 384. 385. 386.\n",
      " 386. 386. 387. 388. 388. 388. 389. 390. 390. 391. 392. 393. 394. 395.\n",
      " 396. 397. 398. 399. 400. 401. 402. 403. 404. 405. 405. 406. 407. 408.\n",
      " 409. 410. 411. 412. 413. 414. 415. 416. 417. 418. 419. 420. 421. 421.\n",
      " 422. 422. 423. 424. 425. 426. 427. 428. 428. 429. 430. 431. 432. 432.\n",
      " 433. 434. 434.]\n",
      "['Vip-Ptprt-Pkp2' 'Vip-Ptprt-Pkp2' 'Vip-Ptprt-Pkp2' 'Vip-Ptprt-Pkp2']\n",
      "[304. 222. 344. 389. 304. 432. 407. 231. 387. 304. 384. 432. 304.]\n"
     ]
    }
   ],
   "source": [
    "# preprocess sessions by sliding windows\n",
    "# determine num_unqiue_cell_types and num_unique_neurons\n",
    "# determine unique_cell_type_ids and unique_neuron_ids\n",
    "\n",
    "# a list of sessions files\n",
    "input_sessions_files = []\n",
    "output_folder = '../../data/Mouse/preprocessed/'\n",
    "\n",
    "directory = '../../data/Mouse/Bugeon/'\n",
    "date_exp = 'SB025/2019-10-07/'\n",
    "input_setting = 'Blank/01/'\n",
    "session_normalization = 'session'\n",
    "\n",
    "accumulate_UnqiueID = []\n",
    "accumulate_cell_type = []\n",
    "\n",
    "activity, frame_times, UniqueID, neuron_ttypes = data.load_mouse_data_session(\n",
    "    directory, date_exp, input_setting, session_normalization\n",
    ")\n",
    "accumulate_UnqiueID.append(UniqueID)\n",
    "accumulate_cell_type.append(neuron_ttypes)\n",
    "\n",
    "# print(UniqueID)\n",
    "# print(neuron_ttypes)\n",
    "\n",
    "directory = '../../data/Mouse/Bugeon/'\n",
    "date_exp = 'SB025/2019-10-04/'\n",
    "input_setting = 'Blank/01/'\n",
    "session_normalization = 'session'\n",
    "\n",
    "activity, frame_times, UniqueID, neuron_ttypes = data.load_mouse_data_session(\n",
    "    directory, date_exp, input_setting, session_normalization\n",
    ")\n",
    "accumulate_UnqiueID.append(UniqueID)\n",
    "accumulate_cell_type.append(neuron_ttypes)\n",
    "\n",
    "directory = '../../data/Mouse/Bugeon/'\n",
    "date_exp = 'SB025/2019-10-08/'\n",
    "input_setting = 'Blank/01/'\n",
    "session_normalization = 'session'\n",
    "\n",
    "activity, frame_times, UniqueID, neuron_ttypes = data.load_mouse_data_session(\n",
    "    directory, date_exp, input_setting, session_normalization\n",
    ")\n",
    "accumulate_UnqiueID.append(UniqueID)\n",
    "accumulate_cell_type.append(neuron_ttypes)\n",
    "\n",
    "directory = '../../data/Mouse/Bugeon/'\n",
    "date_exp = 'SB025/2019-10-09/'\n",
    "input_setting = 'Blank/01/'\n",
    "session_normalization = 'session'\n",
    "\n",
    "activity, frame_times, UniqueID, neuron_ttypes = data.load_mouse_data_session(\n",
    "    directory, date_exp, input_setting, session_normalization\n",
    ")\n",
    "accumulate_UnqiueID.append(UniqueID)\n",
    "accumulate_cell_type.append(neuron_ttypes)\n",
    "\n",
    "directory = '../../data/Mouse/Bugeon/'\n",
    "date_exp = 'SB025/2019-10-23/'\n",
    "input_setting = 'Blank/01/'\n",
    "session_normalization = 'session'\n",
    "\n",
    "activity, frame_times, UniqueID, neuron_ttypes = data.load_mouse_data_session(\n",
    "    directory, date_exp, input_setting, session_normalization\n",
    ")\n",
    "accumulate_UnqiueID.append(UniqueID)\n",
    "accumulate_cell_type.append(neuron_ttypes)\n",
    "\n",
    "directory = '../../data/Mouse/Bugeon/'\n",
    "date_exp = 'SB025/2019-10-24/'\n",
    "input_setting = 'Blank/01/'\n",
    "session_normalization = 'session'\n",
    "\n",
    "activity, frame_times, UniqueID, neuron_ttypes = data.load_mouse_data_session(\n",
    "    directory, date_exp, input_setting, session_normalization\n",
    ")\n",
    "accumulate_UnqiueID.append(UniqueID)\n",
    "accumulate_cell_type.append(neuron_ttypes)\n",
    "\n",
    "# print(activity.shape)\n",
    "# print(UniqueID)\n",
    "# print(neuron_ttypes)\n",
    "\n",
    "# sort UniqueID\n",
    "accumulate_UnqiueID = np.concatenate(accumulate_UnqiueID)\n",
    "accumulate_cell_type = np.concatenate(accumulate_cell_type)\n",
    "print(np.sort(accumulate_UnqiueID[~np.isnan(accumulate_UnqiueID)]))\n",
    "# get the index of accumulated UniqueID = 111\n",
    "print(accumulate_cell_type[np.where(accumulate_UnqiueID == 304)])\n",
    "print(accumulate_UnqiueID[np.where(accumulate_cell_type == 'Vip-Ptprt-Pkp2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126. 127. 128.\n",
      " 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141. 142.\n",
      " 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.  56. 154.  58.\n",
      " 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167. 168.\n",
      " 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181. 182.\n",
      " 183. 184. 185. 186. 187.  48. 188.  72.  73.  77. 189.  86. 190. 191.\n",
      " 192. 193. 194. 195. 196. 197. 198. 199. 200.   1. 201. 202. 203.  11.\n",
      " 204. 205.]\n",
      "[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan 188.  nan 198.\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan 169.  nan 240. 241. 260.  nan 279.  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan   1.  nan  nan  nan  38.\n",
      "  nan  nan]\n"
     ]
    }
   ],
   "source": [
    "def assign_unique_neuron_ids(all_sessions_original_UniqueID, num_neurons_per_session):\n",
    "    \"\"\"\n",
    "    all_sessions_original_UniqueID: a concatenated list of the original UniqueID from all sessions\n",
    "\n",
    "    Return:\n",
    "    all_sessions_new_UniqueID: a list of sessions new UniqueID, each session is a 1D array of shape num_neurons\n",
    "    \"\"\"\n",
    "\n",
    "    # first reassign ID starting from 0 to those non-NaN neurons\n",
    "    # same IDs should be assigned to neurons that have the same original UniqueID\n",
    "    non_nan_values = all_sessions_original_UniqueID[~np.isnan(all_sessions_original_UniqueID)]\n",
    "    unique_non_nan_values = np.unique(non_nan_values)\n",
    "    id_mapping = {unique_non_nan_values[i]: i for i in range(len(unique_non_nan_values))}\n",
    "\n",
    "    new_ids = [id_mapping[non_nan_values[i]] for i in range(len(non_nan_values))]\n",
    "    all_sessions_new_UniqueID = np.copy(all_sessions_original_UniqueID)\n",
    "    all_sessions_new_UniqueID[~np.isnan(all_sessions_new_UniqueID)] = new_ids\n",
    "\n",
    "    # then assign new IDs to those NaN neurons\n",
    "    num_unique_non_nan = unique_non_nan_values.shape[0]     # new IDs start from num_unqiue_non_nan\n",
    "    num_nan = np.sum(np.isnan(all_sessions_original_UniqueID))           # new IDs end with num_non_nan + num_nan -1\n",
    "\n",
    "    new_ids = np.arange(num_unique_non_nan, num_unique_non_nan + num_nan)\n",
    "    all_sessions_new_UniqueID[np.isnan(all_sessions_new_UniqueID)] = new_ids\n",
    "\n",
    "    # Segment all_sessions_new_UniqueID into sessions\n",
    "    all_sessions_new_UniqueID = np.split(all_sessions_new_UniqueID, np.cumsum(num_neurons_per_session)[:-1])\n",
    "\n",
    "    return all_sessions_new_UniqueID     # shape: num_sessions x num_neurons_per_session\n",
    "\n",
    "assign_unique_neuron_ids(UniqueID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['16' '16' '16' '16' '16' '16' '16' '16' '16' '16' '16' '16' '16' '16'\n",
      " '16' '16' '16' '20' '16' '16' '16' '16' '16' '16' '16' '16' '16' '16'\n",
      " '16' '16' '16' '16' '16' '16' '16' '16' '16' '20' '16' '19' '16' '2' '16'\n",
      " '16' '16' '16' '16' '16' '16' '16' '16' '16' '16' '16' '16' '16' '16'\n",
      " '16' '16' '16' '16' '16' '16' '16' '16' '16' '16' '16' '16' '16' '20'\n",
      " '20' '20' '20' '20' '2' '16' '2' '10' '1' '20' '2' '16' '16' '20' '20'\n",
      " '20' '20' '16' '20' '20' '16' '20' '2' '16' '20' '16' '2' '20' '16']\n",
      "['EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'IN', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'IN', 'EC', 'Pvalb-Reln-Itm2a', 'EC', 'Lamp5-Plch2-Dock5', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'EC', 'IN', 'IN', 'IN', 'IN', 'IN', 'Lamp5-Plch2-Dock5', 'EC', 'Lamp5-Plch2-Dock5', 'Lamp5-Ntn1-Npy2r', 'Lamp5-Fam19a1-Tmem182', 'IN', 'Lamp5-Plch2-Dock5', 'EC', 'EC', 'IN', 'IN', 'IN', 'IN', 'EC', 'IN', 'IN', 'EC', 'IN', 'Lamp5-Plch2-Dock5', 'EC', 'IN', 'EC', 'Lamp5-Plch2-Dock5', 'IN', 'EC']\n"
     ]
    }
   ],
   "source": [
    "def assign_unique_cell_type_ids(all_sessions_original_cell_type, num_neurons_per_session):\n",
    "    \"\"\"\n",
    "    all_sessions_original_cell_type: a concatenated list of the original cell types from all sessions (raw cell types)\n",
    "\n",
    "    Return:\n",
    "    all_sessions_new_cell_type: a list of sessions new cell type, each session is a 1D array of shape num_neurons\n",
    "    \"\"\"\n",
    "    # Get the first level of cell types\n",
    "    neuron_types_result = []\n",
    "    for i in range(len(all_sessions_original_cell_type)):\n",
    "        # split by \"-\"\n",
    "        neuron_types_result.append(all_sessions_original_cell_type[i].split(\"-\")[0])\n",
    "    all_sessions_original_cell_type = neuron_types_result\n",
    "    print(all_sessions_original_cell_type[:100])\n",
    "\n",
    "    unique_cell_types = list(set(all_sessions_original_cell_type))\n",
    "    # Assign IDs to cell types\n",
    "    cell_type2id = {unique_cell_types[i]: i for i in range(len(unique_cell_types))}\n",
    "\n",
    "    # Get new cell type IDs\n",
    "    all_sessions_new_cell_type = np.copy(all_sessions_original_cell_type)\n",
    "    for i in range(len(all_sessions_original_cell_type)):\n",
    "        all_sessions_new_cell_type[i] = cell_type2id[all_sessions_original_cell_type[i]]\n",
    "\n",
    "    # Segment all_sessions_new_UniqueID into sessions\n",
    "    all_sessions_new_cell_type = np.split(all_sessions_new_cell_type, np.cumsum(num_neurons_per_session)[:-1])\n",
    "\n",
    "    return all_sessions_new_cell_type, cell_type2id     # shape: num_sessions x num_neurons_per_session\n",
    "    \n",
    "all_sessions_new_cell_type, cell_type2id = assign_unique_cell_type_ids(neuron_ttypes)\n",
    "print(all_sessions_new_cell_type[:100])\n",
    "print(neuron_ttypes[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(all_sessions_acitvity, all_sessions_new_UniqueID, all_sessions_new_cell_type, window_size):\n",
    "    \"\"\"\n",
    "    (can be from train or validation set)\n",
    "    all_sessions_acitvity: a list of sessions activity, each session is a 2D array of shape num_neurons x num_frames\n",
    "    all_sessions_new_UniqueID: a list of sessions new UniqueID, each session is a 1D array of shape num_neurons\n",
    "    all_sessions_new_cell_type: a list of sessions new cell type, each session is a 1D array of shape num_neurons\n",
    "\n",
    "    Return:\n",
    "    - all_sessions_activity_windows:\n",
    "        a list of sessions activity windows, each session is a 3D array of shape num_windows x num_neurons x window_size\n",
    "    - all_sessions_new_UniqueID_windows:\n",
    "        a list of sessions new UniqueID windows, each session is a 2D array of shape num_windows x num_neurons (each row should be the same)\n",
    "    - all_sessions_new_cell_type_windows:\n",
    "        a list of sessions new cell type windows, each session is a 2D array of shape num_windows x num_neurons (each row should be the same)\n",
    "    \"\"\"\n",
    "\n",
    "    all_sessions_activity_windows = []\n",
    "    all_sessions_new_UniqueID_windows = []\n",
    "    all_sessions_new_cell_type_windows = []\n",
    "\n",
    "    for i in range(len(all_sessions_acitvity)):\n",
    "        num_neurons = all_sessions_acitvity[i].shape[0]\n",
    "        num_frames = all_sessions_acitvity[i].shape[1]\n",
    "        num_windows = num_frames - window_size + 1\n",
    "\n",
    "        # activity\n",
    "        activity_windows = np.zeros((num_windows, num_neurons, window_size))\n",
    "        for j in range(num_windows):\n",
    "            activity_windows[j] = all_sessions_acitvity[i][:, j:j+window_size]\n",
    "        all_sessions_activity_windows.append(activity_windows)\n",
    "\n",
    "        # UniqueID\n",
    "        UniqueID_windows = np.zeros((num_windows, num_neurons))\n",
    "        for j in range(num_windows):\n",
    "            UniqueID_windows[j] = all_sessions_new_UniqueID[i]\n",
    "        all_sessions_new_UniqueID_windows.append(UniqueID_windows)\n",
    "\n",
    "        # cell type\n",
    "        cell_type_windows = np.zeros((num_windows, num_neurons))\n",
    "        for j in range(num_windows):\n",
    "            cell_type_windows[j] = all_sessions_new_cell_type[i]\n",
    "        all_sessions_new_cell_type_windows.append(cell_type_windows)\n",
    "\n",
    "    return all_sessions_activity_windows, all_sessions_new_UniqueID_windows, all_sessions_new_cell_type_windows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "func2graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
