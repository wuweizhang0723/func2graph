{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import dirname, join as pjoin\n",
    "import scipy.io as sio\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "from func2graph import data, baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "1 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "2 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "3 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "4 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "5 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "6 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "7 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "8 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "9 torch.Size([3, 10, 100]) torch.Size([3, 10]) torch.Size([3, 10])\n",
      "10 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "11 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "tensor([0.4689, 0.9265, 0.4217, 0.1391, 0.4735, 0.7944, 0.0489, 0.8042, 0.6423,\n",
      "        0.4617, 0.3809, 0.0762, 0.6617])\n",
      "tensor([0.7390, 0.3785, 0.2016, 0.1473, 0.6801, 0.2504, 0.8860, 0.9147, 0.8447,\n",
      "        0.7335, 0.8857, 0.9761, 0.1897])\n",
      "tensor([0.4689, 0.9265, 0.4217, 0.1391, 0.4735, 0.7944, 0.0489, 0.8042, 0.6423,\n",
      "        0.4617, 0.3809, 0.0762, 0.6617])\n",
      "tensor([0.7390, 0.3785, 0.2016, 0.1473, 0.6801, 0.2504, 0.8860, 0.9147, 0.8447,\n",
      "        0.7335, 0.8857, 0.9761, 0.1897])\n",
      "12 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "13 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "14 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "15 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "16 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "17 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "18 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "19 torch.Size([3, 13, 100]) torch.Size([3, 13]) torch.Size([3, 13])\n"
     ]
    }
   ],
   "source": [
    "# sessions are preprocessed by sliding windows, num_unqiue_cell_types and num_unique_neurons are known\n",
    "session1 = torch.rand(32, 10, 100)\n",
    "session2 = torch.rand(30, 13, 100)\n",
    "session1_cell_type_ids = torch.rand(32, 10)\n",
    "session2_cell_type_ids = torch.rand(30, 13)\n",
    "session1_neuron_ids = torch.rand(32, 10)\n",
    "session2_neuron_ids = torch.rand(30, 13)\n",
    "\n",
    "all_sessions = [session1, session2]\n",
    "all_sessions_cell_type_ids = [session1_cell_type_ids, session2_cell_type_ids]\n",
    "all_sessions_neuron_ids = [session1_neuron_ids, session2_neuron_ids]\n",
    "\n",
    "class Mouse_Session_Dataset(TensorDataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        all_sessions, \n",
    "        all_sessions_cell_type_ids, \n",
    "        all_sessions_neuron_ids,\n",
    "        batch_size=3,                      # real batch size\n",
    "    ):\n",
    "        num_batch_per_session = [session.shape[0] // batch_size for session in all_sessions]\n",
    "\n",
    "        self.all_batch = []\n",
    "        self.all_batch_cell_type_ids = []\n",
    "        self.all_batch_neuron_ids = []\n",
    "        for i in range(len(num_batch_per_session)):      # for each session\n",
    "            for j in range(num_batch_per_session[i]):      # for each batch\n",
    "                self.all_batch.append(all_sessions[i][j*batch_size:(j+1)*batch_size])\n",
    "                self.all_batch_cell_type_ids.append(all_sessions_cell_type_ids[i][j*batch_size:(j+1)*batch_size])\n",
    "                self.all_batch_neuron_ids.append(all_sessions_neuron_ids[i][j*batch_size:(j+1)*batch_size])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.all_batch[index], self.all_batch_cell_type_ids[index], self.all_batch_neuron_ids[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_batch)\n",
    "\n",
    "\n",
    "\n",
    "dataset = Session_Dataset(all_sessions, all_sessions_cell_type_ids, all_sessions_neuron_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)    # this is not real batch_size\n",
    "for idx, (batch, batch_cell_type_ids, batch_neuron_ids) in enumerate(dataloader):\n",
    "    batch = batch.squeeze(0)                 # remove the fake batch_size\n",
    "    batch_cell_type_ids = batch_cell_type_ids.squeeze(0)\n",
    "    batch_neuron_ids = batch_neuron_ids.squeeze(0)\n",
    "    print(idx, batch.shape, batch_cell_type_ids.shape, batch_neuron_ids.shape)\n",
    "\n",
    "    if idx == 11:\n",
    "        print(batch_cell_type_ids[1])\n",
    "        print(batch_neuron_ids[1])\n",
    "        print(session2_cell_type_ids[4])\n",
    "        print(session2_neuron_ids[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "  28.  29.  30.  31.  32.  33.  34.  34.  35.  35.  36.  37.  38.  39.\n",
      "  40.  41.  41.  41.  42.  43.  44.  45.  45.  45.  45.  46.  47.  47.\n",
      "  48.  48.  48.  48.  49.  50.  51.  52.  53.  53.  53.  53.  54.  55.\n",
      "  56.  57.  58.  58.  59.  59.  60.  60.  61.  61.  62.  63.  64.  65.\n",
      "  65.  65.  66.  67.  68.  69.  70.  71.  72.  72.  73.  74.  75.  75.\n",
      "  76.  77.  78.  79.  79.  80.  81.  82.  82.  83.  83.  84.  85.  85.\n",
      "  85.  85.  86.  87.  87.  88.  88.  89.  90.  91.  91.  92.  92.  93.\n",
      "  93.  94.  95.  96.  96.  96.  97.  98.  98.  98.  98.  99. 100. 100.\n",
      " 101. 101. 101. 102. 102. 102. 103. 103. 103. 104. 105. 106. 107. 108.\n",
      " 108. 108. 109. 110. 110. 111. 111. 112. 113. 114. 115. 116. 117. 117.\n",
      " 117. 117. 118. 118. 118. 119. 120. 120. 121. 122. 123. 124. 125. 126.\n",
      " 127. 127. 128. 129. 130. 131. 131. 132. 132. 133. 133. 133. 134. 134.\n",
      " 135. 136. 137. 138. 139. 139. 139. 140. 140. 141. 142. 142. 143. 144.\n",
      " 145. 146. 146. 147. 147. 148. 149. 149. 149. 150. 151. 152. 152. 153.\n",
      " 153. 154. 154. 154. 154. 155. 156. 157. 157. 158. 158. 158. 159. 160.\n",
      " 161. 162. 163. 163. 164. 165. 166. 166. 166. 167. 168. 169. 169. 170.\n",
      " 170. 171. 171. 172. 172. 172. 173. 173. 174. 175. 176. 177. 177. 177.\n",
      " 177. 178. 179. 180. 181. 181. 182. 183. 184. 184. 184. 184. 185. 186.\n",
      " 187. 187. 187. 188. 189. 190. 190. 191. 192. 192. 193. 194. 194. 195.\n",
      " 196. 197. 197. 198. 199. 200. 201. 201. 202. 203. 204. 204. 205. 205.\n",
      " 205. 206. 207. 208. 208. 209. 210. 210. 211. 212. 212. 212. 213. 213.\n",
      " 213. 214. 215. 215. 215. 216. 216. 217. 217. 217. 218. 219. 220. 220.\n",
      " 221. 222. 223. 223. 224. 225. 225. 226. 226. 227. 227. 228. 228. 229.\n",
      " 230. 231. 232. 233. 234. 235. 236. 237. 237. 238. 239. 240. 240. 240.\n",
      " 241. 242. 242. 242. 243. 244. 244. 245. 246. 246. 247. 248. 248. 249.\n",
      " 250. 251. 252. 252. 252. 253. 254. 255. 256. 257. 258. 259. 260. 260.\n",
      " 261. 261. 262. 263. 264. 265. 266. 267. 268. 269. 269. 270. 270. 271.\n",
      " 272. 273. 273. 274. 275. 275. 275. 276. 277. 278. 279. 280. 281. 282.\n",
      " 283. 284. 285. 285. 286. 287. 287. 288. 288. 288. 289. 289. 289. 290.\n",
      " 291. 292. 293. 294. 295. 296. 296. 297. 298. 299. 299. 299. 300. 300.\n",
      " 301. 301. 302. 303. 304. 304. 304. 304. 305. 305. 305. 306. 306. 306.\n",
      " 306. 307. 308. 308. 309. 310. 311. 312. 312. 312. 313. 314. 314. 314.\n",
      " 314. 315. 316. 317. 318. 318. 318. 318. 319. 320. 321. 321. 322. 322.\n",
      " 323. 324. 325. 326. 326. 326. 327. 327. 327. 328. 329. 329. 330. 330.\n",
      " 331. 332. 333. 334. 335. 336. 337. 337. 337. 338. 338. 339. 340. 341.\n",
      " 342. 343. 344. 345. 346. 347. 348. 349. 350. 350. 351. 351. 352. 352.\n",
      " 352. 353. 354. 355. 356. 356. 357. 357. 358. 358. 359. 360. 361. 362.\n",
      " 363. 363. 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375.\n",
      " 376. 377. 378. 379. 379. 380. 380. 381. 382. 383. 383. 384. 385. 386.\n",
      " 386. 386. 387. 388. 388. 388. 389. 390. 390. 391. 392. 393. 394. 395.\n",
      " 396. 397. 398. 399. 400. 401. 402. 403. 404. 405. 405. 406. 407. 408.\n",
      " 409. 410. 411. 412. 413. 414. 415. 416. 417. 418. 419. 420. 421. 421.\n",
      " 422. 422. 423. 424. 425. 426. 427. 428. 428. 429. 430. 431. 432. 432.\n",
      " 433. 434. 434.]\n"
     ]
    }
   ],
   "source": [
    "# preprocess sessions by sliding windows\n",
    "# determine num_unqiue_cell_types and num_unique_neurons\n",
    "# determine unique_cell_type_ids and unique_neuron_ids\n",
    "\n",
    "# a list of sessions files\n",
    "input_sessions_files = []\n",
    "output_folder = '../../data/Mouse/preprocessed/'\n",
    "\n",
    "directory = '../../data/Mouse/Bugeon/'\n",
    "date_exp = 'SB025/2019-10-07/'\n",
    "input_setting = 'Blank/01/'\n",
    "session_normalization = 'session'\n",
    "\n",
    "accumulate_UnqiueID = []\n",
    "\n",
    "activity, frame_times, UniqueID, neuron_ttypes = data.load_mouse_data_session(\n",
    "    directory, date_exp, input_setting, session_normalization\n",
    ")\n",
    "accumulate_UnqiueID.append(UniqueID)\n",
    "\n",
    "directory = '../../data/Mouse/Bugeon/'\n",
    "date_exp = 'SB025/2019-10-04/'\n",
    "input_setting = 'Blank/01/'\n",
    "session_normalization = 'session'\n",
    "\n",
    "activity, frame_times, UniqueID, neuron_ttypes = data.load_mouse_data_session(\n",
    "    directory, date_exp, input_setting, session_normalization\n",
    ")\n",
    "accumulate_UnqiueID.append(UniqueID)\n",
    "\n",
    "directory = '../../data/Mouse/Bugeon/'\n",
    "date_exp = 'SB025/2019-10-08/'\n",
    "input_setting = 'Blank/01/'\n",
    "session_normalization = 'session'\n",
    "\n",
    "activity, frame_times, UniqueID, neuron_ttypes = data.load_mouse_data_session(\n",
    "    directory, date_exp, input_setting, session_normalization\n",
    ")\n",
    "accumulate_UnqiueID.append(UniqueID)\n",
    "\n",
    "directory = '../../data/Mouse/Bugeon/'\n",
    "date_exp = 'SB025/2019-10-09/'\n",
    "input_setting = 'Blank/01/'\n",
    "session_normalization = 'session'\n",
    "\n",
    "activity, frame_times, UniqueID, neuron_ttypes = data.load_mouse_data_session(\n",
    "    directory, date_exp, input_setting, session_normalization\n",
    ")\n",
    "accumulate_UnqiueID.append(UniqueID)\n",
    "\n",
    "directory = '../../data/Mouse/Bugeon/'\n",
    "date_exp = 'SB025/2019-10-23/'\n",
    "input_setting = 'Blank/01/'\n",
    "session_normalization = 'session'\n",
    "\n",
    "activity, frame_times, UniqueID, neuron_ttypes = data.load_mouse_data_session(\n",
    "    directory, date_exp, input_setting, session_normalization\n",
    ")\n",
    "accumulate_UnqiueID.append(UniqueID)\n",
    "\n",
    "directory = '../../data/Mouse/Bugeon/'\n",
    "date_exp = 'SB025/2019-10-24/'\n",
    "input_setting = 'Blank/01/'\n",
    "session_normalization = 'session'\n",
    "\n",
    "activity, frame_times, UniqueID, neuron_ttypes = data.load_mouse_data_session(\n",
    "    directory, date_exp, input_setting, session_normalization\n",
    ")\n",
    "accumulate_UnqiueID.append(UniqueID)\n",
    "\n",
    "# print(activity.shape)\n",
    "# print(UniqueID)\n",
    "# print(neuron_ttypes)\n",
    "\n",
    "# sort UniqueID\n",
    "accumulate_UnqiueID = np.concatenate(accumulate_UnqiueID)\n",
    "print(np.sort(accumulate_UnqiueID[~np.isnan(accumulate_UnqiueID)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   2.   3.   4.   5.   7.  22.  24.  26.  27.  38.  45.  48.\n",
      "  53.  57.  61.  62.  69.  70.  71.  78.  85.  87.  88.  95.  98. 101.\n",
      " 102. 104. 111. 112. 117. 118. 123. 133. 139. 149. 153. 154. 156. 157.\n",
      " 158. 159. 160. 162. 166. 168. 169. 170. 172. 177. 178. 179. 184. 187.\n",
      " 188. 196. 198. 199. 201. 205. 207. 208. 212. 213. 216. 217. 218. 220.\n",
      " 223. 237. 240. 241. 242. 248. 252. 260. 261. 265. 269. 273. 275. 276.\n",
      " 277. 278. 279. 280. 281. 285. 287. 288. 289. 296. 298. 299. 300. 301.\n",
      " 304. 305. 306. 312. 314. 318. 326. 327. 334. 339. 346. 352. 356. 357.\n",
      " 358. 361. 363.]\n",
      "115\n",
      "[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan 188.  nan 198.\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan 169.  nan 240. 241. 260.  nan 279.  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan   1.  nan  nan  nan  38.\n",
      "  nan  nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan 56. nan 58. nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan 48. nan 72. 73. 77. nan 86. nan nan nan nan nan nan nan nan\n",
      " nan nan nan  1. nan nan nan 11. nan nan]\n",
      "[115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126. 127. 128.\n",
      " 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141. 142.\n",
      " 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.  56. 154.  58.\n",
      " 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167. 168.\n",
      " 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181. 182.\n",
      " 183. 184. 185. 186. 187.  48. 188.  72.  73.  77. 189.  86. 190. 191.\n",
      " 192. 193. 194. 195. 196. 197. 198. 199. 200.   1. 201. 202. 203.  11.\n",
      " 204. 205.]\n",
      "495\n",
      "495\n"
     ]
    }
   ],
   "source": [
    "def assign_unique_neuron_ids(all_sessions_original_UniqueID, num_neurons_per_session):\n",
    "    \"\"\"\n",
    "    all_sessions_original_UniqueID: a list of UniqueID from all sessions\n",
    "    num_neurons_per_session: a list of number of neurons from all sessions\n",
    "    \"\"\"\n",
    "\n",
    "    # first reassign ID starting from 0 to those non-NaN neurons\n",
    "    # same IDs should be assigned to neurons that have the same UniqueID\n",
    "    non_nan_values = all_sessions_original_UniqueID[~np.isnan(all_sessions_original_UniqueID)]\n",
    "\n",
    "    unique_non_nan_values = np.unique(non_nan_values)\n",
    "    print(unique_non_nan_values)\n",
    "    print(len(unique_non_nan_values))\n",
    "    id_mapping = {unique_non_nan_values[i]: i for i in range(len(unique_non_nan_values))}\n",
    "\n",
    "    new_ids = [id_mapping[non_nan_values[i]] for i in range(len(non_nan_values))]\n",
    "    all_sessions_new_UniqueID = np.copy(all_sessions_original_UniqueID)\n",
    "    all_sessions_new_UniqueID[~np.isnan(all_sessions_new_UniqueID)] = new_ids\n",
    "\n",
    "    print(all_sessions_original_UniqueID[:100])\n",
    "    print(all_sessions_new_UniqueID[:100])\n",
    "\n",
    "    # then assign new IDs to those NaN neurons\n",
    "    num_non_nan = np.sum(~np.isnan(all_sessions_original_UniqueID))     # new IDs start from num_non_nan\n",
    "    num_nan = np.sum(np.isnan(all_sessions_original_UniqueID))           # new IDs end with num_non_nan + num_nan -1\n",
    "\n",
    "    new_ids = np.arange(num_non_nan, num_non_nan + num_nan)\n",
    "    all_sessions_new_UniqueID[np.isnan(all_sessions_new_UniqueID)] = new_ids\n",
    "\n",
    "    print(all_sessions_new_UniqueID[:100])\n",
    "\n",
    "    # check if the number of unique ids is the same as the length of array\n",
    "    unique_ids = np.unique(all_sessions_new_UniqueID)\n",
    "    print(len(unique_ids))\n",
    "    print(len(all_sessions_new_UniqueID))\n",
    "\n",
    "\n",
    "assign_unique_neuron_ids(UniqueID, [len(UniqueID)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "func2graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
